# E-Commerce Customer Segmentation & Churn Prediction using RFM and Machine Learning

## 📌 Project Overview
This repository contains an end-to-end pipeline for **e-commerce customer analytics**: cleaning transaction data, computing **RFM (Recency, Frequency, Monetary)** scores, deriving customer segments, engineering temporal features, training churn-prediction models (LightGBM as primary), evaluating with a **rolling-origin** strategy, and deploying a simple **Gradio** app to run predictions on new transaction CSVs. This notebook and codebase were designed to be reproducible and production-friendly — with clear preprocessing, feature engineering, model training, evaluation, and an app for inference.

---

## 🔎 What’s included
- Data cleaning utilities for typical e-commerce transactional data.
- RFM computation and quantile-based scoring.
- KMeans segmentation and cluster profiling.
- Rolling-window feature engineering (30/90/180 days) for churn prediction.
- Churn label generation (churn = no purchase in N days after cutoff).
- Model training with LightGBM (and optional baselines).
- Rolling-origin evaluation (temporal splits) and AUC reporting.
- Gradio app for uploading CSV and returning churn predictions and RFM segments.
- Export capabilities: `final_data.csv` (RFM results) and `churn_model.txt` (LightGBM).

---

## ⚙️ Tech stack
- Python 3.8+  
- pandas, numpy, scikit-learn, lightgbm, xgboost (optional), matplotlib, seaborn, plotly  
- Gradio (for demo app)  
- Jupyter Notebook (analysis & visualizations)

---

## 📂 Project structure
E-Commerce-RFM-Churn/
├── data/
│ ├── E-com_Data.csv # raw transactional data (example)
│ └── final_data.csv # RFM export generated by the notebook
├── notebooks/
│ └── E-Commerce Domain - RFM Approach .ipynb # main analysis notebook
├── README.md # this file
└── models/
└── churn_model.txt # saved LightGBM model (optional)

---

## 🗂️ Data expectations / schema
The pipeline expects transaction-level records with at least the following columns (column names handled/fixed by `clean_transactions()` if common typos exist):
- `CustomerID` (int or string)  
- `InvoiceNo` (invoice id; unique invoice per transaction line or grouped)  
- `Date` or `Date of purchase` (YYYY-MM-DD; parsed to datetime)  
- `Quantity` (integer, defaults to 1 if missing)  
- `Price` (float; this pipeline computes `LineTotal = Price * Quantity` — confirm if `Price` already means line total in your dataset)  
- `Item Code` (optional, used for per-cluster item aggregation)  

**Notes:**  
- The notebook contains a robust `clean_transactions(df)` function which normalizes column names (`InvoieNo` → `InvoiceNo`, `Date of purchase` → `Date`), converts types, computes `LineTotal`, drops rows missing critical fields, removes future-dated records, and marks returns via negative `LineTotal` (`is_return`).

---

## 📊 RFM approach and scoring
1. Compute per-customer aggregates:
   - `Recency` = days since last purchase relative to `Latest_date`.  
   - `Frequency` = count of invoices (choose `nunique(InvoiceNo)` if invoices can have multiple lines).  
   - `Monetary` = sum of `Price` (or `LineTotal` if `Price` is per-unit).  
2. Compute quartiles (25/50/75) across customers.  
3. Assign scores:  
   - `R`: lower recency → better → score 1..4 (1 = best).  
   - `F`, `M`: higher is better → mapping inverted so 1 = best, 4 = worst.  
4. `RFMvalue` = sum(R + F + M) — **lower values are better** (best customer might be 3 = 1+1+1).  
5. `RFMGroup` = string concatenation like `"112"` to identify pattern.  
6. `Loyalty_Level` (quartile-based labels e.g., Platinum, Diamond, Gold, Silver) assigned via `pd.qcut` on `RFMvalue`.

---

## 🤖 Clustering & segmentation
- StandardScaler is applied to `Recency`, `Frequency`, `Monetary` before clustering.  
- Elbow method (inertia) is used to pick `k` (range 2..10).  
- `KMeans(n_clusters=4)` was used as a stable example.  
- Cluster-level profiles (mean R/F/M, top products) are computed to interpret customer groups.  

---

## 🛠️ Feature engineering for churn prediction
The churn modeling pipeline includes:
- `churn_label_by_cutoff(df, cutoff_date, churn_days=90)`: labels any customer with zero purchases in the next `churn_days` as churned.  
- `build_features_for_cutoff(df_before, cutoff_date, windows=[30,90,180])`:  
  - Features: last purchase, first purchase, frequency, monetary, avg order value, return count.  
  - Rolling-window features for each window (frequency, monetary, avg order, unique items).  
  - Recency, tenure, return rate.  
- NaNs are filled; non-numeric columns are dropped before modeling.  

---

## 📈 Modeling & evaluation
- Rolling-origin evaluation is implemented with temporal cutoffs.  
- At each cutoff: build `X` and `y`, drop customers with insufficient labels, train, validate.  
- Models included:  
  - **LightGBM** (primary; with early stopping).  
  - RandomForest (baseline).  
  - LogisticRegression (baseline).  
- Example LightGBM params:  
  ```python
  params = {
      "objective": "binary",
      "metric": "auc",
      "num_leaves": 31,
      "min_data_in_leaf": 50,
      "feature_fraction": 0.8,
      "bagging_fraction": 0.8,
      "bagging_freq": 5,
      "lambda_l1": 0.1,
      "lambda_l2": 0.1
  }
   ```

---

 ## 🖥️ Gradio app
Upload CSV with required schema.  

App cleans data, computes features, loads pretrained LightGBM, and outputs churn predictions + RFM segmentation.  

Example UI includes customer-level churn probability and loyalty segment.  

---

## 📦 Exports
- `final_data.csv`: enriched dataset with RFM scores and loyalty levels.  
- `churn_model.txt`: trained LightGBM model saved in native format.  

---

## 📌 Notes & caveats
- Ensure date column parsing is correct (especially if mixing `Date` and `Date of purchase`).  
- Returns are handled via negative line totals — confirm matches your schema.  
- Frequency metric may be defined as invoice count vs. transaction count; adjust as needed.  
- Ensure model retraining periodically, since churn dynamics shift over time.

---
  
## 🙌 Acknowledgments
- RFM methodology widely used in marketing analytics.  
- LightGBM open-source community.  


  



